{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN THIS NOTEBOOK, SAMPLE SEGMENTS ARE SELECTED FROM THE DIARIZATION OUTCOME AND CORRESPONGDING FEATURES ARE EXTRACTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: select sample segments, randomly select 150 segments from each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:09:51.662639Z",
     "iopub.status.busy": "2024-09-13T15:09:51.662076Z",
     "iopub.status.idle": "2024-09-13T15:09:51.700710Z",
     "shell.execute_reply": "2024-09-13T15:09:51.700062Z",
     "shell.execute_reply.started": "2024-09-13T15:09:51.662588Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "segments_df = pd.read_csv(\"/home/zmi/data/processed_data/diarization_outcome.csv\")\n",
    "\n",
    "selected_segments = []\n",
    "\n",
    "grouped = segments_df.groupby('file_name')\n",
    "for name, group in grouped:\n",
    "    if len(group) < 150:\n",
    "        raise ValueError(f\"Not enough segments to sample 100 segments from file: {name}\")\n",
    "    selected_segments.append(group.sample(n=150))\n",
    "\n",
    "#concatenate all the sampled segments into one DataFrame\n",
    "selected_segments_df = pd.concat(selected_segments, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEP 2: extract MFCC, x-vectors and embeddings of selected segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:26:51.057489Z",
     "iopub.status.busy": "2024-09-13T15:26:51.057145Z",
     "iopub.status.idle": "2024-09-13T15:26:51.062805Z",
     "shell.execute_reply": "2024-09-13T15:26:51.061934Z",
     "shell.execute_reply.started": "2024-09-13T15:26:51.057465Z"
    }
   },
   "outputs": [],
   "source": [
    "#prepare the file path\n",
    "import os\n",
    "\n",
    "directory_path = \"/data/zmi/train_L/wav\"\n",
    "\n",
    "audio_file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:56.261028Z",
     "iopub.status.busy": "2024-09-13T15:54:56.260414Z",
     "iopub.status.idle": "2024-09-13T16:21:52.878485Z",
     "shell.execute_reply": "2024-09-13T16:21:52.877437Z",
     "shell.execute_reply.started": "2024-09-13T15:54:56.260973Z"
    }
   },
   "outputs": [],
   "source": [
    "#extract the features\n",
    "#the difference between x-vector and embeddings here are: extracted based on different models,x-vector capture global speaker identity across an entire segment,\n",
    "#while embeddings are more granular, can capture both global and local speaker characteristics\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pyannote.audio import Inference\n",
    "from pyannote.audio.models.embedding.xvector import XVectorMFCC\n",
    "\n",
    "def extract_segment_mfcc(y, sr, start_time, end_time, n_mfcc=13, n_fft=320):\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    y_segment = y[start_sample:end_sample]\n",
    "\n",
    "    #ensure n_fft is not larger than the segment length\n",
    "    segment_length = len(y_segment)\n",
    "    n_fft = min(320, segment_length // 2)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "    return mfccs\n",
    "\n",
    "embedding_model = Inference(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "xvector_model = XVectorMFCC(sample_rate=16000)\n",
    "\n",
    "MIN_AUDIO_LENGTH = 3000\n",
    "\n",
    "file_path_mapping = {os.path.basename(path): path for path in audio_file_paths}\n",
    "\n",
    "segment_info = []\n",
    "\n",
    "for file_name in selected_segments_df['file_name'].unique():\n",
    "    file_path = file_path_mapping.get(file_name)\n",
    "    if file_path is None:\n",
    "        print(f\"File path for {file_name} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    #load the audio file using for mfcc extraction\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    #load the audio file for embedding extraction\n",
    "    waveform, sr_torch = torchaudio.load(file_path)\n",
    "\n",
    "    #get the segments corresponding to this file\n",
    "    segments = selected_segments_df[selected_segments_df['file_name'] == file_name]\n",
    "\n",
    "    for _, segment in segments.iterrows():\n",
    "        start_time = segment['start_time']\n",
    "        end_time = segment['end_time']\n",
    "        speaker = segment['speaker']\n",
    "\n",
    "        #extract the corresponding audio segment for embedding and x-vector extraction\n",
    "        start_sample = int(start_time * sr_torch)\n",
    "        end_sample = int(end_time * sr_torch)\n",
    "        segment_audio = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        #ensure the segment has the minimum required number of samples for the x-vector model\n",
    "        if segment_audio.shape[1] < MIN_AUDIO_LENGTH:\n",
    "            padding_needed = MIN_AUDIO_LENGTH - segment_audio.shape[1]\n",
    "            segment_audio = torch.nn.functional.pad(segment_audio, (0, padding_needed))\n",
    "\n",
    "        #extract the first channel (x-vector expects single-channel audio)\n",
    "        segment_audio = segment_audio[0, :]\n",
    "\n",
    "        #ensure the tensor has the correct shape (batch_size, num_channels, num_samples)\n",
    "        if segment_audio.ndim == 1:\n",
    "            segment_audio = segment_audio.unsqueeze(0)  #add channel dimension\n",
    "        if segment_audio.ndim == 2:\n",
    "            segment_audio = segment_audio.unsqueeze(0)  #add batch dimension\n",
    "\n",
    "        print(f\"Segment audio shape before model: {segment_audio.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            xvector = xvector_model(segment_audio)\n",
    "\n",
    "        embedding = embedding_model({\"waveform\": segment_audio.squeeze(0), \"sample_rate\": sr_torch})\n",
    "\n",
    "        mfcc = extract_segment_mfcc(y, sr, start_time, end_time)\n",
    "\n",
    "        #ensure embedding and xvector are tensors before converting them to numpy arrays\n",
    "        if isinstance(embedding.data, torch.Tensor):\n",
    "            embedding_np = embedding.data.cpu().numpy()\n",
    "        else:\n",
    "            embedding_np = embedding.data\n",
    "\n",
    "        if isinstance(xvector, torch.Tensor):\n",
    "            xvector_np = xvector.cpu().numpy()\n",
    "        else:\n",
    "            xvector_np = xvector\n",
    "\n",
    "        segment_info.append({\n",
    "            'file': file_name,\n",
    "            'Speaker': speaker,\n",
    "            'mfcc': mfcc,\n",
    "            'embedding': embedding_np,\n",
    "            'xvector': xvector_np\n",
    "        })\n",
    "\n",
    "segment_df = pd.DataFrame(segment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: store the extracted features for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T16:50:25.101598Z",
     "iopub.status.busy": "2024-09-13T16:50:25.101014Z",
     "iopub.status.idle": "2024-09-13T16:50:46.878997Z",
     "shell.execute_reply": "2024-09-13T16:50:46.878404Z",
     "shell.execute_reply.started": "2024-09-13T16:50:25.101551Z"
    }
   },
   "outputs": [],
   "source": [
    "segment_df.to_csv('features_selected_segments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
